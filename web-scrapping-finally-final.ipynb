{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the data of world's top insurance companies by `market capitalization`\n",
    "\n",
    "![](https://i.imgur.com/lA4LqC7.png)\n",
    "\n",
    "\n",
    "**Data** is the collection of facts!\n",
    "\n",
    "_**Web Scraping**_ is a technique used to automatically extract large amounts of data from websites and save it to a file or database. The data scraped will usually be in tabular or spreadsheet format(e.g : CSV file)\n",
    "\n",
    "\n",
    "Here, in this web scrapping we will scrap data from [value.today](https://www.value.today/world-top-companies/insurance).\n",
    "\n",
    "We'll use the Python libraries `requests` and `beautifulsoup4` to perform scrapping from the webpage.\n",
    "\n",
    "\n",
    "\n",
    "Here's an outline of the steps we'll follow:\n",
    "\n",
    "1. Download the webpage using `requests`\n",
    "2. Parse the HTML source code using `beautifulsoup4`\n",
    "3. Extract company names, CEOs, world ranks, Market capitalization, Annual revenue, number of employees, company URLs\n",
    "4. Compile the extracted information into and Python lists and dictionaries\n",
    "5. Extract and combine data from multiple pages\n",
    "6. Save the extracted information to a CSV file.\n",
    "\n",
    "\n",
    "By the end of the project, we'll create a CSV file in the following format:\n",
    "\n",
    "![](https://i.imgur.com/a0Bllr6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run the Code\n",
    "You can execute the code using the \"Run\" button at the top of this page using \"Run on binder\". You can make changes and save your version of the notebook to [Jovian](https://www.jovian.ai) by executing the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "#jovian.commit(project=\"web-scrapping-finally-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the webpage using `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `requests` library to download the web page.\n",
    "\n",
    "The library can be installed using `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library is now installed and imported.\n",
    "\n",
    "To download a page, we can use the `get` function from requests, which returns a response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://www.value.today/world-top-companies/insurance'\n",
    "response = requests.get(topics_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests.get` returns a response object containing the data from the web page and some other information.\n",
    "\n",
    "The `.status_code` property can be used to check if the response was successful. A successful response will have an [HTTP status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) between 200 and 299."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request was successful. We can get the contents of the page using `response.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of characters of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161485"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contains over 160000 characters!\n",
    "\n",
    "Here are the first 1000 characters of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# \">\\n  <head>\\n    <meta charset=\"utf-8\"/>\\n<script async src=\"//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script>\\n<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:\"ca-pub-2407955258669770\",enable_page_level_ads:true});</script><script>window.google_analytics_uacct=\"UA-121331115-1\";(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"https://www'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell `page_content[:1000]` contains the [HTML](https://en.wikipedia.org/wiki/HTML) of the webpage [value.today](https://www.value.today/world-top-companies/insurance)\n",
    "\n",
    "We can also save it to a file and view the page locally within Jupyter using \"File > Open\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('world-insurance.html','w',encoding = \"utf-8\") as file:\n",
    "    file.write(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page looks similar to the original page\n",
    "\n",
    "![](https://i.imgur.com/Kmmbnlo.png)\n",
    "\n",
    "In the section, we used the requests library to download a web page as HTML. We have successfully downloaded the webpage using `requests` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the HTML source code using `beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `doc` object, we can navigate and search through the `HTML` for data that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc` object contains several properties and methods for extracting information from the HTML document.\n",
    "\n",
    "[the documentation of BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>World Top Insurance Companies by Market Value as on 2021</title>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `doc.find('title')` will give the title of the web page.\n",
    "\n",
    "![](https://i.imgur.com/of9igZA.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a web page and return a beautiful soup doc\"\"\"\n",
    "    # Download the webpage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the dowmload was successful\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Unable to download page {}'.format(url))\n",
    "    \n",
    "    # Get the page HTML\n",
    "    page_contents = response.text\n",
    "    \n",
    "    # Create a bs4 doc\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = get_page(topics_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>World Top Insurance Companies by Market Value as on 2021</title>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc` and `doc2` has the same title `World Top Insurance Companies by Market Value as on 2021`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `get_page` to downlaod any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract company names, CEOs, world ranks, Market capitalization, Annual revenue, number of employees, company URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the box we get an idea that all the information that we need to scrape is under `li` tag with `class` attribute set to `row well clearfix`\n",
    "\n",
    "![](https://i.imgur.com/i3Srr2K.png)\n",
    "\n",
    "Let's find all the `li` tags matching this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_block = doc.find_all('li',class_='row well clearfix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web page contain 10 boxes of `li` tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's create a `function` to extract the all the company names of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_of_companies(company_block):\n",
    "    company_names = []\n",
    "    for tag in company_block:\n",
    "        c_name = tag.find('div',class_='field field--name-node-title field--type-ds field--label-hidden field--item')\n",
    "\n",
    "        company_names.append(c_name.find('a').text)\n",
    "    return company_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `name_of_companies` to get the companies names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERKSHIRE HATHAWAY',\n",
       " 'UNITEDHEALTH GROUP',\n",
       " 'BANK OF AMERICA CORPORATION',\n",
       " 'WELLS FARGO & COMPANY',\n",
       " 'AIA GROUP',\n",
       " 'ROYAL BANK OF CANADA',\n",
       " 'PING AN INSURANCE (GROUP) COMPANY OF CHINA',\n",
       " 'BANK OF CHINA',\n",
       " 'STATE FARM',\n",
       " 'TORONTO-DOMINION BANK']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the function\n",
    "name_of_companies(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the `CEO` is under the `div` of class `field--item` of `href` attribute\n",
    "\n",
    "![](https://i.imgur.com/UxAUb2n.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `function` to extract the all the CEO names of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_of_CEOs(company_block):\n",
    "    CEO_names = []\n",
    "    for tag in company_block:\n",
    "        names = tag.find('div',class_='clearfix col-sm-12 field field--name-field-ceo field--type-entity-reference field--label-above')\n",
    "        try:\n",
    "            ceo = names.find('a').text\n",
    "            CEO_names.append(ceo)\n",
    "        except AttributeError:\n",
    "            CEO_names.append(None)\n",
    "    return CEO_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `name_of_CEOs` to get the CEOs name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Warren Buffett',\n",
       " 'David S. Wichmann',\n",
       " 'Brian Moynihan',\n",
       " 'Charles W. Scharf',\n",
       " 'Lee Yuan Siong',\n",
       " 'David I. McKay',\n",
       " 'Ma Mingzhe',\n",
       " 'Gao Yingxin',\n",
       " None,\n",
       " 'Bharat Masrani']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the function\n",
    "name_of_CEOs(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `function` to extract the all the `world ranks` of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranks_of_world(company_block):\n",
    "    world_ranks = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        rank = tag.find('div', class_='clearfix col-sm-6 field field--name-field-world-rank-sep-01-2021- field--type-integer field--label-above')\n",
    "\n",
    "        world_ranks.append(rank.find('div',class_='field--item').text)\n",
    "    return world_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `ranks_of_world` to get the `world ranks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8', '18', '20', '65', '91', '93', '94', '111', '131', '133']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks_of_world(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `function` to extract the all the `market capitalization' of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_caps(company_block):\n",
    "    market_capitalization_in_dollars = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        market_cap = tag.find('div',class_='clearfix col-sm-6 field field--name-field-market-value-jan012021 field--type-float field--label-above')\n",
    "        try:\n",
    "            caps = market_cap.find('div',class_='field--item').text\n",
    "            replace_caps = caps.replace(' Billion USD',\"\")\n",
    "            market_capitalization_in_dollars.append(float(replace_caps))\n",
    "        except AttributeError:\n",
    "            market_capitalization_in_dollars.append(None)\n",
    "    return market_capitalization_in_dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `market_caps` to get the `market capitalization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[543.68, 332.73, 262.2, 124.78, 152.33, 116.72, 233.34, 129.25, None, 102.4]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the function\n",
    "market_caps(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anuual Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `function` to extract the all the `annual revenue` of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_rev(company_block):\n",
    "    annual_revenue_in_dollars = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        annual_revenue = tag.find('div',class_='clearfix col-sm-12 field field--name-field-revenue-in-usd field--type-float field--label-inline')\n",
    "        try:\n",
    "            revenue = annual_revenue.find('div',class_='field--item').text\n",
    "            replace_string = revenue.replace(',',\"\").replace(' Million USD',\"\")\n",
    "\n",
    "            annual_revenue_in_dollars.append(int(replace_string))\n",
    "\n",
    "        except AttributeError:\n",
    "            annual_revenue_in_dollars.append(None)\n",
    "    return annual_revenue_in_dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `annual_rev` to get the `annual revenue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[286260, 255630, 85530, 72340, 50360, 37367, 166950, 82215, 81730, 34568]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the function\n",
    "annual_rev(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `function` to extract the all the `employees` of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employees(company_block):\n",
    "    no_of_employees = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        employee = tag.find('div',class_='clearfix col-sm-12 field field--name-field-employee-count field--type-integer field--label-inline')\n",
    "        try:\n",
    "            n_employee = employee.find('div',class_='field--item').text\n",
    "            replace_string = n_employee.replace(',',\"\")\n",
    "            no_of_employees.append(int(replace_string))\n",
    "        except AttributeError:\n",
    "            no_of_employees.append(None)\n",
    "    return no_of_employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `employees` to get the `number of employees`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[391500, 320000, 208000, 258700, 23000, 83842, 376900, 309384, 59000, 89598]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the function\n",
    "employees(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/umXAZPT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `function` to extract the all the `Company URLs` of first page using the `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(company_block):    \n",
    "    company_urls = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        c_url = tag.find('div',class_='clearfix col-sm-12 field field--name-field-company-website field--type-link field--label-above')\n",
    "        try:\n",
    "\n",
    "            company_urls.append(c_url.find('a')['href'])\n",
    "        except AttributeError:\n",
    "            company_urls.append(None)\n",
    "    return company_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function `extract_urls` to get the `Company URLs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.berkshirehathaway.com/',\n",
       " 'https://www.unitedhealthgroup.com/',\n",
       " 'https://www.bankofamerica.com/',\n",
       " 'https://www.wellsfargo.com/',\n",
       " 'http://www.aia.com/',\n",
       " 'https://www.rbcroyalbank.com',\n",
       " 'http://www.pingan.com/',\n",
       " 'https://www.boc.cn/en/',\n",
       " 'https://www.statefarm.com/',\n",
       " 'https://www.td.com']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_urls(company_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have created `7` function. These are `name_of_companies`, `name_of_CEOs`, `ranks_of_world`, `market_caps`, `annual_rev`, `employees`, `extract_urls`. And now we have developed an approach to extract the data from a block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the extracted information into and Python lists and dictionaries\n",
    "\n",
    "## Extract and combine data from multiple pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `dictionary` using all the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/cPlfU16.png)\n",
    "\n",
    "As there is `53` pages on wesite, We will need to `loop` through all the pages. So that we can extract the data from all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page():\n",
    "    all_info_dict = {}\n",
    "   \n",
    "    all_info_dict = {\n",
    "            'companies_name':[],\n",
    "            'CEOs_name':[],\n",
    "            'world_ranks':[],\n",
    "            'market_capitalizations_in_billion_dollars':[],\n",
    "            'annual_revenues_in_million_dollars':[],\n",
    "            'number_of_employees':[],\n",
    "            'companies_URLs':[]\n",
    "            }\n",
    "    for page in range (0,53):\n",
    "\n",
    "        url = f\"https://www.value.today/world-top-companies/insurance?title=&field_headquarters_of_company_target_id&field_company_category_primary_target_id&field_company_website_uri=&field_market_cap_aug_01_2021__value=&page={page}\"\n",
    "        company_block = get_page(url).find_all('li',class_='row well clearfix')\n",
    "\n",
    "        all_info_dict['companies_name'] += name_of_companies(company_block)\n",
    "        all_info_dict['CEOs_name'] += name_of_CEOs(company_block)\n",
    "        all_info_dict['world_ranks'] += ranks_of_world(company_block)\n",
    "        all_info_dict['market_capitalizations_in_billion_dollars'] += market_caps(company_block)\n",
    "        all_info_dict['annual_revenues_in_million_dollars'] += annual_rev(company_block)\n",
    "        all_info_dict['number_of_employees'] += employees(company_block)\n",
    "        all_info_dict['companies_URLs'] += extract_urls(company_block)\n",
    "        page = page + 1\n",
    "    return all_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe from dictionary\n",
    "import pandas as pd\n",
    "scrape_page_dataframe = pd.DataFrame(scrape_page())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companies_name</th>\n",
       "      <th>CEOs_name</th>\n",
       "      <th>world_ranks</th>\n",
       "      <th>market_capitalizations_in_billion_dollars</th>\n",
       "      <th>annual_revenues_in_million_dollars</th>\n",
       "      <th>number_of_employees</th>\n",
       "      <th>companies_URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERKSHIRE HATHAWAY</td>\n",
       "      <td>Warren Buffett</td>\n",
       "      <td>8</td>\n",
       "      <td>543.680</td>\n",
       "      <td>286260.0</td>\n",
       "      <td>391500.0</td>\n",
       "      <td>https://www.berkshirehathaway.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNITEDHEALTH GROUP</td>\n",
       "      <td>David S. Wichmann</td>\n",
       "      <td>18</td>\n",
       "      <td>332.730</td>\n",
       "      <td>255630.0</td>\n",
       "      <td>320000.0</td>\n",
       "      <td>https://www.unitedhealthgroup.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BANK OF AMERICA CORPORATION</td>\n",
       "      <td>Brian Moynihan</td>\n",
       "      <td>20</td>\n",
       "      <td>262.200</td>\n",
       "      <td>85530.0</td>\n",
       "      <td>208000.0</td>\n",
       "      <td>https://www.bankofamerica.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELLS FARGO &amp; COMPANY</td>\n",
       "      <td>Charles W. Scharf</td>\n",
       "      <td>65</td>\n",
       "      <td>124.780</td>\n",
       "      <td>72340.0</td>\n",
       "      <td>258700.0</td>\n",
       "      <td>https://www.wellsfargo.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AIA GROUP</td>\n",
       "      <td>Lee Yuan Siong</td>\n",
       "      <td>91</td>\n",
       "      <td>152.330</td>\n",
       "      <td>50360.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>http://www.aia.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>NOVUS ACQUISITION &amp; DEVELOPMENT</td>\n",
       "      <td>None</td>\n",
       "      <td>36,479</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>INSR INSURANCE GROUP ASA</td>\n",
       "      <td>None</td>\n",
       "      <td>36,703</td>\n",
       "      <td>0.010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>ATLAS FINANCIAL HOLDINGS, INC.</td>\n",
       "      <td>None</td>\n",
       "      <td>37,108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>HEALTH REVENUE ASSURANCE HOLDINGS, INC.</td>\n",
       "      <td>None</td>\n",
       "      <td>37,682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>TRIAD GUARANTY</td>\n",
       "      <td>None</td>\n",
       "      <td>38,342</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>527 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              companies_name          CEOs_name world_ranks  \\\n",
       "0                         BERKSHIRE HATHAWAY     Warren Buffett           8   \n",
       "1                         UNITEDHEALTH GROUP  David S. Wichmann          18   \n",
       "2                BANK OF AMERICA CORPORATION     Brian Moynihan          20   \n",
       "3                      WELLS FARGO & COMPANY  Charles W. Scharf          65   \n",
       "4                                  AIA GROUP     Lee Yuan Siong          91   \n",
       "..                                       ...                ...         ...   \n",
       "522          NOVUS ACQUISITION & DEVELOPMENT               None      36,479   \n",
       "523                 INSR INSURANCE GROUP ASA               None      36,703   \n",
       "524           ATLAS FINANCIAL HOLDINGS, INC.               None      37,108   \n",
       "525  HEALTH REVENUE ASSURANCE HOLDINGS, INC.               None      37,682   \n",
       "526                           TRIAD GUARANTY               None      38,342   \n",
       "\n",
       "     market_capitalizations_in_billion_dollars  \\\n",
       "0                                      543.680   \n",
       "1                                      332.730   \n",
       "2                                      262.200   \n",
       "3                                      124.780   \n",
       "4                                      152.330   \n",
       "..                                         ...   \n",
       "522                                      0.005   \n",
       "523                                      0.010   \n",
       "524                                        NaN   \n",
       "525                                        NaN   \n",
       "526                                      0.001   \n",
       "\n",
       "     annual_revenues_in_million_dollars  number_of_employees  \\\n",
       "0                              286260.0             391500.0   \n",
       "1                              255630.0             320000.0   \n",
       "2                               85530.0             208000.0   \n",
       "3                               72340.0             258700.0   \n",
       "4                               50360.0              23000.0   \n",
       "..                                  ...                  ...   \n",
       "522                                 NaN                  NaN   \n",
       "523                                 NaN                  NaN   \n",
       "524                                 NaN                  NaN   \n",
       "525                                 NaN                  NaN   \n",
       "526                                 NaN                  NaN   \n",
       "\n",
       "                         companies_URLs  \n",
       "0    https://www.berkshirehathaway.com/  \n",
       "1    https://www.unitedhealthgroup.com/  \n",
       "2        https://www.bankofamerica.com/  \n",
       "3           https://www.wellsfargo.com/  \n",
       "4                   http://www.aia.com/  \n",
       "..                                  ...  \n",
       "522                                None  \n",
       "523                                None  \n",
       "524                                None  \n",
       "525                                None  \n",
       "526                                None  \n",
       "\n",
       "[527 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view the first and last 5 rows\n",
    "scrape_page_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the extracted information to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_page_dataframe.to_csv('scrape_page_dataframe.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's what we've covered in this notebook\n",
    "\n",
    "1. Downloaded the webpage using `requests`\n",
    "2. Parsed the HTML source code using `beautifulsoup4`\n",
    "3. Extracted company names, CEOs, world ranks, Market capitalization, Annual revenue, number of employees, company URLs\n",
    "4. Compiled the extracted information into and Python lists and dictionaries\n",
    "5. Extracted and combine data from multiple pages\n",
    "6. Saved the extracted information to a CSV file.\n",
    "\n",
    "\n",
    "The CSV file we created has this format:\n",
    "\n",
    "![](https://i.imgur.com/a0Bllr6.png)\n",
    "\n",
    "Here's the complete code for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a web page and return a beautiful soup doc\"\"\"\n",
    "    # Download the webpage\n",
    "    response = requests.get(url)    \n",
    "    # Check if the dowmload was successful\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Unable to download page {}'.format(url))    \n",
    "    # Get the page HTML\n",
    "    page_contents = response.text    \n",
    "    # Create a bs4 doc\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return doc\n",
    "\n",
    "\n",
    "def name_of_companies(company_block):\n",
    "    company_names = []\n",
    "    for tag in company_block:\n",
    "        c_name = tag.find('div',class_='field field--name-node-title field--type-ds field--label-hidden field--item')\n",
    "\n",
    "        company_names.append(c_name.find('a').text)\n",
    "    return company_names\n",
    "\n",
    "\n",
    "def name_of_CEOs(company_block):\n",
    "    CEO_names = []\n",
    "    for tag in company_block:\n",
    "        names = tag.find('div',class_='clearfix col-sm-12 field field--name-field-ceo field--type-entity-reference field--label-above')\n",
    "        try:\n",
    "            ceo = names.find('a').text\n",
    "            CEO_names.append(ceo)\n",
    "        except AttributeError:\n",
    "            CEO_names.append(None)\n",
    "    return CEO_names\n",
    "\n",
    "\n",
    "def ranks_of_world(company_block):\n",
    "    world_ranks = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        rank = tag.find('div', class_='clearfix col-sm-6 field field--name-field-world-rank-sep-01-2021- field--type-integer field--label-above')\n",
    "\n",
    "        world_ranks.append(rank.find('div',class_='field--item').text)\n",
    "    return world_ranks\n",
    "\n",
    "\n",
    "def market_caps(company_block):\n",
    "    market_capitalization_in_dollars = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        market_cap = tag.find('div',class_='clearfix col-sm-6 field field--name-field-market-value-jan012021 field--type-float field--label-above')\n",
    "        try:\n",
    "            caps = market_cap.find('div',class_='field--item').text\n",
    "            replace_caps = caps.replace(' Billion USD',\"\")\n",
    "            market_capitalization_in_dollars.append(float(replace_caps))\n",
    "        except AttributeError:\n",
    "            market_capitalization_in_dollars.append(None)\n",
    "    return market_capitalization_in_dollars\n",
    "\n",
    "\n",
    "def annual_rev(company_block):\n",
    "    annual_revenue_in_dollars = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        annual_revenue = tag.find('div',class_='clearfix col-sm-12 field field--name-field-revenue-in-usd field--type-float field--label-inline')\n",
    "        try:\n",
    "            revenue = annual_revenue.find('div',class_='field--item').text\n",
    "            replace_string = revenue.replace(',',\"\").replace(' Million USD',\"\")\n",
    "\n",
    "            annual_revenue_in_dollars.append(int(replace_string))\n",
    "\n",
    "        except AttributeError:\n",
    "            annual_revenue_in_dollars.append(None)\n",
    "    return annual_revenue_in_dollars\n",
    "\n",
    "\n",
    "def employees(company_block):\n",
    "    no_of_employees = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        employee = tag.find('div',class_='clearfix col-sm-12 field field--name-field-employee-count field--type-integer field--label-inline')\n",
    "        try:\n",
    "            n_employee = employee.find('div',class_='field--item').text\n",
    "            replace_string = n_employee.replace(',',\"\")\n",
    "            no_of_employees.append(int(replace_string))\n",
    "        except AttributeError:\n",
    "            no_of_employees.append(None)\n",
    "    return no_of_employees\n",
    "\n",
    "\n",
    "\n",
    "def extract_urls(company_block):    \n",
    "    company_urls = []\n",
    "\n",
    "    for tag in company_block:\n",
    "        c_url = tag.find('div',class_='clearfix col-sm-12 field field--name-field-company-website field--type-link field--label-above')\n",
    "        try:\n",
    "\n",
    "            company_urls.append(c_url.find('a')['href'])\n",
    "        except AttributeError:\n",
    "            company_urls.append(None)\n",
    "    return company_urls\n",
    "\n",
    "\n",
    "\n",
    "def scrape_page():\n",
    "    all_info_dict = {}\n",
    "   \n",
    "    all_info_dict = {\n",
    "            'companies_name':[],\n",
    "            'CEOs_name':[],\n",
    "            'world_ranks':[],\n",
    "            'market_capitalizations':[],\n",
    "            'annual_revenues':[],\n",
    "            'number_of_employees':[],\n",
    "            'companies_URLs':[]\n",
    "            }\n",
    "    for page in range (0,53):\n",
    "\n",
    "        url = f\"https://www.value.today/world-top-companies/insurance?title=&field_headquarters_of_company_target_id&field_company_category_primary_target_id&field_company_website_uri=&field_market_cap_aug_01_2021__value=&page={page}\"\n",
    "        company_block = get_page(url).find_all('li',class_='row well clearfix')\n",
    "\n",
    "        all_info_dict['companies_name'] += name_of_companies(company_block)\n",
    "        all_info_dict['CEOs_name'] += name_of_CEOs(company_block)\n",
    "        all_info_dict['world_ranks'] += ranks_of_world(company_block)\n",
    "        all_info_dict['market_capitalizations_in_billion_dollars'] += market_caps(company_block)\n",
    "        all_info_dict['annual_revenues_in_million_dollars'] += annual_rev(company_block)\n",
    "        all_info_dict['number_of_employees'] += employees(company_block)\n",
    "        all_info_dict['companies_URLs'] += extract_urls(company_block)\n",
    "        page = page + 1\n",
    "    return all_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "* We can now fetch individual topic pages, and get the list of top insurance companies \n",
    "* We can scrape the page to get the additioanal information\n",
    "* We can use this data for further analysis\n",
    "* We can extract the data of two or more different audit month and perform the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Jovian](https://jovian.ai/) A platform to learn Data Science\n",
    "\n",
    "* This project is made under the guidence of [Aakash N S](https://aakashns.medium.com/) \n",
    "\n",
    "* A Youtube video by `Aakash N S` [Let's Build a Python Web Scraping Project from Scratch | Hands-On Tutorial](https://www.youtube.com/watch?v=RKsLLG-bzEY&t=6677s)\n",
    "\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "                                 \n",
    "* [Pandas Documentation](https://pandas.pydata.org/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"pankajthakur3999/web-scrapping-of-top-insurance-companies\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/pankajthakur3999/web-scrapping-of-top-insurance-companies\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/pankajthakur3999/web-scrapping-of-top-insurance-companies'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(files = ['scrape_page_dataframe.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}